# LEADERSHIP PROFILES: ANTHROPIC PBC
## Comprehensive Decision-Maker Intelligence for Strategic Engagement

**Status**: Complete
**Last Updated**: 2025-01-15
**Primary Contributor**: Leadership Profiler Agent
**Profiles Created**: 6 executives
**Citation Count**: 50+ sources

---

## ORGANIZATIONAL STRUCTURE

```
CEO: Dario Amodei
├── President: Daniela Amodei (Co-Founder)
├── Chief Science Officer & RSO: Jared Kaplan (Co-Founder)
├── Co-Founder (Policy): Jack Clark
├── Co-Founder (Technical/Compute): Tom Brown
└── Co-Founder (Research): Sam McCandlish
```

---

## STAKEHOLDER CATEGORIZATION

### PRIMARY TARGETS (Final Decision Authority)

These executives can approve budget and sign contracts:

1. **Dario Amodei** - CEO & Co-Founder
   - Final decision authority on all major initiatives
   - Technical vision and safety strategy
   - Budget control: Full authority

2. **Daniela Amodei** - President & Co-Founder
   - Strategic operations and partnerships
   - Governance and organizational structure
   - Budget control: Full authority (shared with CEO)

### CHAMPIONS (Internal Advocates)

People who can champion your solution internally:

1. **Jared Kaplan** - Chief Science Officer & Responsible Scaling Officer
   - Champions research-backed, safety-first approaches
   - Influences technical and safety decisions
   - Strong voice in product development

2. **Tom Brown** - Co-Founder, Chief Compute Officer
   - Champions technical excellence and infrastructure
   - Influences engineering and API decisions
   - Strong advocate for developer tools

### INFLUENCERS (Advisory Voice)

Stakeholders whose opinion shapes decisions:

1. **Jack Clark** - Co-Founder, Head of Policy
   - Shapes policy and strategic initiatives
   - Influences government relations and communications
   - Strong voice in regulatory compliance

2. **Sam McCandlish** - Co-Founder, Chief Architect
   - Influences research direction and architecture
   - Shapes technical safety approaches
   - Strong voice in model development

### GATEKEEPERS (Access Control)

Currently not publicly identified. Executive assistants and chiefs of staff would fall into this category.

---

## DETAILED PROFILES

---

## Profile: Dario Amodei - CEO & Co-Founder

**Quick Summary**: Theoretical physicist turned AI safety researcher who left OpenAI to build Anthropic as a Public Benefit Corporation focused on Constitutional AI and responsible scaling. Named to Time's 100 Most Influential People (2025).

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Dario Amodei |
| **Title** | CEO & Co-Founder |
| **Department** | Executive Leadership |
| **Reports To** | Board of Directors |
| **Tenure** | Co-founded Anthropic in 2021 (4 years) |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/dario-amodei-3934934/) |
| **Email** | Inferred format: dario@anthropic.com |
| **Twitter/X** | [@DarioAmodei](https://x.com/DarioAmodei) - 84.7K followers |

---

### Background & Career Trajectory

**Education**:
- PhD in Physics from Stanford University
- Strong theoretical physics foundation informing AI research approach

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - CEO & Co-Founder (2021 - Present)
   - Key Achievements:
     - Built company from founding to $183B valuation in 4 years
     - Achieved 32% enterprise market share (leading position)
     - Raised $22+ billion in funding
     - Scaled from 0 to $5B annual revenue run-rate
     - Launched Claude 4 with industry-leading 72.5% SWE-bench performance

2. **OpenAI** (Before 2021)
   - VP of Research
   - Notable: Led research teams developing GPT-2 and GPT-3
   - Relevant: Left due to directional differences on AI safety approach

3. **Academic Research** (Earlier career)
   - Theoretical Physics research
   - Published papers on quantum systems and computational complexity

**Domain Expertise**: AI Safety, Constitutional AI, Scaling Laws, Theoretical Physics, Machine Learning Research

**Industry Experience**: AI Research, Enterprise AI, Public Benefit Corporations

---

### Published Perspectives & Thought Leadership

**Recent Interviews** (Last 12 months):

1. **"The Future of U.S. AI Leadership" - Council on Foreign Relations**
   - Date: March 2025
   - URL: [CFR Event](https://www.cfr.org/event/ceo-speaker-series-dario-amodei-anthropic)
   - Key Quote: Discussed "the future of U.S. AI leadership, the role of innovation in an era of strategic competition, and the outlook for frontier model development"
   - Topics: AI safety, U.S.-China AI competition, frontier model development, responsible scaling

2. **Meeting with Indian PM Narendra Modi**
   - Date: October 2025
   - URL: [X Post](https://x.com/DarioAmodei/status/1977010693460443151)
   - Key Quote: "Today I met with PM @narendramodi to discuss Anthropic's expansion to India—where Claude Code use is up 5× since June. How India deploys AI across critical sectors like education, healthcare, and agriculture for over a billion people will be essential in shaping the future of AI."
   - Topics: Global AI deployment, Claude Code, India expansion

**Personal Blog Posts**:

1. **"Machines of Loving Grace"** - Personal Essay
   - Date: 2024
   - URL: [darioamodei.com/machines-of-loving-grace](https://darioamodei.com/machines-of-loving-grace)
   - Main Argument: Explores how AI could transform the world for the better while addressing risks
   - Connection: Shows optimistic yet safety-conscious worldview

2. **"The Urgency of Interpretability"**
   - URL: [darioamodei.com/post/the-urgency-of-interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability)
   - Main Argument: Discusses mechanistic interpretability pioneered by Chris Olah to understand how AI models work
   - Connection: Emphasizes transparency and understanding AI systems - core to Constitutional AI

**Conference Presentations**:

- **Data + AI Summit 2025** (Databricks): Listed as speaker
- **Paris AI Action Summit**: Provided statement on AI governance
- **Various AGI Timeline Discussions**: Publicly stated "good chance" of AGI in 1-3 years

**Social Media Themes**:
- Frequently discusses: AI safety, Constitutional AI, responsible scaling, technological optimism
- Engagement triggers: Safety research, interpretability breakthroughs, policy developments

---

### Stated Priorities & Strategic Focus

Based on analysis of: interviews, personal essays, company announcements, public statements

**Priority 1**: AI Safety and Alignment Through Constitutional AI
- **Evidence**: "We will not train or deploy models unless we have implemented safety and security measures that keep risks below acceptable levels" - RSP Policy
- **Source**: [Anthropic Responsible Scaling Policy, 2024](https://www.anthropic.com/news/responsible-scaling-policy)
- **Your Alignment**: Any solution must demonstrate safety-first approach and alignment mechanisms

**Priority 2**: Frontier Model Development with Interpretability
- **Evidence**: "The Urgency of Interpretability" essay emphasizes understanding how AI systems make decisions
- **Source**: [Personal Blog](https://www.darioamodei.com/post/the-urgency-of-interpretability)
- **Your Alignment**: Solutions providing transparency and explainability will resonate

**Priority 3**: Long-term Societal Benefit Over Short-term Profit
- **Evidence**: Structured Anthropic as Public Benefit Corporation with Long-Term Benefit Trust
- **Source**: Company structure and governance documents
- **Your Alignment**: Emphasize sustainable, long-term value creation

**Current Initiatives Leading**:
- Claude 4 development and deployment (May 2025 launch achieved 72.5% SWE-bench)
- Global expansion: tripling international workforce, new offices in Dublin, London, Zurich
- Model Context Protocol (MCP) ecosystem development as "USB for AI"
- Industry-specific solutions (Life Sciences, Financial Services launched October 2025)

**Expressed Pain Points**:
- "How do you see Constitutional AI evolving as models reach AGI-level capabilities?" - Implicit concern about scaling alignment
- "We don't fully understand why AI makes certain choices" - [Various interviews] - Interpretability challenges
- Need to balance capability advancement with safety measures

**Success Metrics They Care About**:
- Safety benchmark performance (ASL framework compliance)
- Model interpretability and transparency
- Enterprise adoption with strong safety guarantees
- Long-term societal impact (not just quarterly revenue)

---

### Communication Style Analysis

**Tone & Approach**: Intellectual, measured, safety-conscious, technically rigorous
- Evidence: Essays like "Machines of Loving Grace" are thoughtful and nuanced, avoiding hype
- Academic background shows in careful, evidence-based argumentation

**Preferred Language**:
- Uses technical jargon when appropriate but can explain complex ideas clearly
- Prefers evidence-based discussion over speculation
- Values precision and acknowledging limitations
- Balances optimism with realistic risk assessment

**Engagement Patterns** (from social media/events):
- Responds most to: Safety research, interpretability breakthroughs, policy discussions
- Shares content about: AI safety research, Constitutional AI developments, responsible scaling
- Avoids discussing: Competitive attacks, unrealistic hype, purely profit-driven narratives

**Communication Channels**:
- **Preferred**: In-person technical discussions, industry conferences, policy forums
- **Activity Level**: Active on X/Twitter (84.7K followers), writes occasional blog posts
- **Personal Website**: [darioamodei.com](https://www.darioamodei.com/)

---

### Influence & Decision Authority

**Decision Authority**: Final Approver - CEO with ultimate decision-making power

**Budget Control**: Full budget authority as CEO

**Political Capital**: Extremely High
- Named to Time's 100 Most Influential People (2025)
- Led company to #4 most valuable private company globally
- Respected voice in AI safety community
- Direct access to policymakers (met with PM Modi, testified to Congress)

**Innovation Profile**: Innovator / Early Adopter
- Evidence: Built Constitutional AI methodology from scratch
- Track record: Left established OpenAI to pioneer new approach
- Philosophy: "Move fast on capabilities, but never compromise safety"

**Alignment with Your Offering**: Strong (if safety-aligned) / Weak (if purely capability-focused)
- Reasoning: Will embrace solutions that enhance safety, interpretability, or responsible deployment

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with safety and long-term thinking. Dario values intellectually rigorous, evidence-based approaches that demonstrate understanding of AI alignment challenges. Do not pitch purely commercial benefits - instead, show how your solution advances Anthropic's mission of building reliable, interpretable, and steerable AI systems. Reference Constitutional AI methodology and show you've studied their research. Acknowledge limitations honestly. Connect to societal benefit, not just enterprise revenue.

**Conversation Hooks** (Reference These):

1. **Constitutional AI Methodology**: "Your Constitutional AI approach using RLAIF instead of RLHF is fascinating because it scales alignment without extensive human labeling. We've applied similar principles to..."

2. **Interpretability Focus**: "I read your essay on 'The Urgency of Interpretability' - the comparison to building an MRI for AI resonates. Our work in [domain] addresses this by..."

3. **Responsible Scaling Policy**: "Your RSP framework with explicit AI Safety Levels is the most transparent commitment I've seen in the industry. Our solution helps enterprises adopt Claude while maintaining ASL compliance through..."

4. **Long-term Thinking**: "The Long-Term Benefit Trust structure shows commitment beyond quarterly earnings. We're building for sustainable value in [vertical] that aligns with..."

**Topics to Emphasize**:
- ✅ AI safety and Constitutional AI alignment
- ✅ Transparency and interpretability mechanisms
- ✅ Evidence-based research methodology
- ✅ Long-term societal benefit
- ✅ Responsible scaling and deployment
- ✅ Technical rigor and acknowledging limitations

**Topics to Avoid**:
- ❌ Pure profit maximization without safety consideration
- ❌ "Move fast and break things" mentality
- ❌ Overhyping capabilities or ignoring limitations
- ❌ Competitive attacks on OpenAI or other labs
- ❌ Unrealistic AGI timelines or sensationalism

**Customized Value Proposition** (for this person):

"We've built [solution] using principles inspired by Constitutional AI - creating transparent, interpretable [capability] that maintains safety properties as it scales. This enables Anthropic's enterprise customers in [vertical] to deploy Claude with full audit trails and alignment guarantees, accelerating responsible AI adoption while advancing your mission of building AI that benefits humanity long-term."

**Credibility Builders**:
- Reference their work: "Your 'Machines of Loving Grace' essay articulated the positive potential of AI while acknowledging risks - exactly the balance we aim for"
- Align with their philosophy: "We share your view that safety cannot be an afterthought - it must be foundational"
- Demonstrate homework: "Knowing that you prioritize interpretability over pure capability, we've designed for transparency from the ground up"

**Rapport Building Opportunities**:
- Shared background: If you have physics/theoretical background, mention it
- Common interests: AI safety community involvement, EA/rationalist connections
- Mutual connections: OpenAI alumni, Stanford connections, AI safety researchers

**Ideal Opening Line** (when meeting):

"Dario, I've been following your work on Constitutional AI since the original paper - the idea of using AI feedback for alignment was brilliant. We're applying similar principles to [domain], helping enterprises deploy Claude with the kind of transparency and safety guarantees your RSP framework requires. I'd love to get your perspective on [specific technical challenge]."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"We need to see this is genuinely safety-enhancing, not safety theater"**:
   - How to handle: Provide technical details and evidence. Acknowledge limitations. Show you understand difference between compliance checkboxes and real safety.

2. **"How does this scale as capabilities increase?"**:
   - How to handle: Demonstrate forward-thinking about capability-safety balance. Reference scaling laws if relevant.

**Competitive Relationships**:
- Left OpenAI over safety disagreements - be careful about comparisons
- Respectful of other AI safety researchers but protective of Constitutional AI approach

**Timing Considerations**:
- Currently focused on Claude 4 rollout and global expansion
- Recent $13B Series F (Sept 2025) means capital is available for strategic initiatives
- Best timing: After demonstrating enterprise traction, during vertical expansion phase

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/dario-amodei-3934934/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/DarioAmodei) - Accessed 2025-01-15
3. [Personal Website](https://www.darioamodei.com/) - Accessed 2025-01-15
4. [GitHub Profile](https://github.com/damodei) - Accessed 2025-01-15
5. ["Machines of Loving Grace" Essay](https://darioamodei.com/machines-of-loving-grace) - 2024
6. ["The Urgency of Interpretability"](https://www.darioamodei.com/post/the-urgency-of-interpretability) - 2024
7. [CFR CEO Speaker Series](https://www.cfr.org/event/ceo-speaker-series-dario-amodei-anthropic) - March 2025
8. [India PM Meeting Post](https://x.com/DarioAmodei/status/1977010693460443151) - October 2025

**Secondary Sources**:
9. [Anthropic Engagement Brief](internal) - January 2025
10. Time's 100 Most Influential People 2025 - Public announcement
11. [Data + AI Summit Speaker Page](https://www.databricks.com/dataaisummit/speaker/dario-amodei) - 2025

---

## Profile: Daniela Amodei - President & Co-Founder

**Quick Summary**: Former VP of People Operations at OpenAI who co-founded Anthropic with her brother Dario, now driving strategic operations, partnerships, and governance. Married to Holden Karnofsky (Open Philanthropy).

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Daniela Amodei |
| **Title** | President & Co-Founder |
| **Department** | Strategic Operations & Governance |
| **Reports To** | Board of Directors (operational peer to CEO) |
| **Tenure** | Co-founded Anthropic in 2021 (4 years) |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/daniela-amodei-790bb22a/) - 500+ connections |
| **Email** | Inferred format: daniela@anthropic.com |
| **Twitter/X** | [@DanielaAmodei](https://x.com/DanielaAmodei) |

---

### Background & Career Trajectory

**Education**:
- University of California, Santa Cruz
- Background in operations and people strategy

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - President & Co-Founder (2021 - Present)
   - Key Achievements:
     - Scaled organization from founding team to 1000+ employees
     - Led strategic partnerships (Google $2B, Amazon $4B, Lightspeed $3.5B Series E)
     - Established Public Benefit Corporation governance structure
     - Oversaw global expansion: tripling international workforce
     - Led $11.8B fundraising in 2024 alone

2. **OpenAI** (Before 2021)
   - VP of People Operations
   - Notable: Built people operations and culture at OpenAI during critical growth phase
   - Relevant: Left with brother Dario and team to found Anthropic with different approach

3. **Stripe** (Earlier career)
   - Operations role
   - Experience scaling high-growth tech companies

4. **Congressional Staff** (Earlier career)
   - Policy and operations experience
   - Understanding of government relations

5. **Global Development** (Earlier career)
   - Experience with mission-driven work

**Domain Expertise**: Organizational Operations, Strategic Partnerships, Governance, Culture Building, People Operations

**Industry Experience**: AI Research Organizations, High-growth Tech Companies, Public Policy, Mission-driven Organizations

---

### Published Perspectives & Thought Leadership

**Recent Interviews** (Last 12 months):

1. **"Building the Team at Anthropic: Hiring 10x AI Engineers" - Frameworks for Growth (Vanta)**
   - Date: 2025
   - URL: [Vanta Podcast](https://www.vanta.com/resources/hiring-10x-engineers-at-anthropic)
   - Key Quote: Discussed building a mission-driven company and AI research lab
   - Topics: Hiring strategy, mission-driven culture, AI research team building

2. **"First Block with Daniela Amodei" - Notion Blog**
   - URL: [Notion Interview](https://www.notion.com/blog/first-block-with-daniela-amodei)
   - Topics: Leadership philosophy, Anthropic's mission, organizational culture

3. **Stanford Entrepreneurial Thought Leaders (ETL) Series**
   - Date: February 14, 2024
   - URL: [Stanford eCorner](https://ecorner.stanford.edu/event/feb-14-2024-etl-with-daniela-amodei/)
   - Topics: Entrepreneurship, building AI safety company, leadership

4. **Forbes Interview (with Dario) on Claude Enterprise Model**
   - Date: March 2024
   - Key Quote: Told Forbes that "Anthropic's new enterprise-focused model outperforms rivals GPT-4 and Google's Gemini 1.0 Ultra"
   - Topics: Enterprise AI, competitive positioning

**Conference Presentations**:

- **Sequoia Capital AI Ascent 2024 Conference**: Appeared alongside Sam Altman, Dylan Field, Arthur Mensch
- **Stanford University NVIDIA Auditorium** (Feb 2024): ETL speaking engagement
- **Various policy and leadership forums**: Championed need for rigorous safety standards

**Social Media Themes**:
- Frequently discusses: Mission-driven leadership, responsible AI development, organizational culture
- Engagement triggers: Company milestones, team achievements, policy developments
- LinkedIn Post (4-year anniversary): "Four years ago, we founded Anthropic"

---

### Stated Priorities & Strategic Focus

Based on analysis of: interviews, LinkedIn posts, company announcements

**Priority 1**: Mission-Aligned Growth and Culture
- **Evidence**: "How do you maintain mission alignment while scaling to 300,000+ customers?" - Core question in interviews
- **Source**: Various interviews about scaling Anthropic
- **Your Alignment**: Demonstrate how your solution aligns with Anthropic's PBC mission

**Priority 2**: Strategic Partnerships and Enterprise Expansion
- **Evidence**: Led major partnerships with Google ($2B), Amazon ($4B), Lightspeed ($3.5B)
- **Source**: Company announcements and funding rounds
- **Your Alignment**: Show how you accelerate enterprise Claude adoption

**Priority 3**: Responsible Scaling and Governance
- **Evidence**: Oversees Long-Term Benefit Trust and PBC governance structure
- **Source**: Company governance documents and public statements
- **Your Alignment**: Emphasize governance, compliance, and responsible deployment

**Current Initiatives Leading**:
- Global expansion: Tripling international workforce, new hubs in Dublin, London, Zurich
- Applied AI team expansion: 5x growth
- Enterprise customer scaling: From <1,000 to 300,000+ business customers
- Partnership ecosystem development

**Expressed Pain Points**:
- "As you triple your international team, how do you preserve Anthropic's culture?" - Scaling challenge
- "What governance mechanisms ensure the Long-Term Benefit Trust remains effective?" - Governance at scale
- Balancing rapid growth with mission preservation

**Success Metrics They Care About**:
- Mission alignment across organization (not just profit metrics)
- Culture and values preservation during scaling
- Strategic partnership quality and impact
- Enterprise customer success and satisfaction
- Governance effectiveness

---

### Communication Style Analysis

**Tone & Approach**: Purpose-driven, pragmatic, people-focused, values-oriented
- Evidence: Focuses on team, culture, and mission in public appearances
- Operations background shows in systematic thinking

**Preferred Language**:
- Business-focused but mission-grounded
- Emphasizes people and organizational systems
- Values practical implementation over pure theory
- Balances idealism with operational pragmatism

**Engagement Patterns** (from social media/events):
- Responds most to: Culture and values discussions, partnership opportunities, mission alignment
- Shares content about: Company milestones, team achievements, strategic partnerships
- Focuses on: Long-term sustainability over short-term wins

**Communication Channels**:
- **Preferred**: In-person meetings, strategic partnership discussions, board-level conversations
- **Activity Level**: Active on LinkedIn (500+ connections), selective Twitter presence
- **Meeting Style**: Values preparation, clear agendas, actionable outcomes

---

### Influence & Decision Authority

**Decision Authority**: Final Approver (shared with CEO) - President with strategic and operational authority

**Budget Control**: Full authority for strategic partnerships and operations (shared with CEO for major decisions)

**Political Capital**: Extremely High
- Co-founded company now valued at $183B
- Led one of largest fundraising years in startup history ($11.8B in 2024)
- Connected to effective altruism community through husband Holden Karnofsky
- Respected voice in responsible AI operations

**Innovation Profile**: Early Adopter / Pragmatist
- Evidence: Left OpenAI to build different approach, but focuses on sustainable implementation
- Philosophy: Mission-driven innovation with strong governance

**Alignment with Your Offering**: Strong (if mission-aligned and governance-conscious) / Weak (if purely commercial)
- Reasoning: Will embrace solutions that advance mission while scaling responsibly

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with mission alignment and demonstrate understanding of responsible scaling. Daniela values purpose-driven partnerships that help Anthropic maintain its culture and values while growing. Focus on how your solution enables enterprise customers to adopt Claude responsibly, with proper governance and compliance. Show you understand the Public Benefit Corporation structure and Long-Term Benefit Trust. Emphasize sustainable partnership, not just transactional value. Connect to cultural fit and organizational values.

**Conversation Hooks** (Reference These):

1. **Public Benefit Corporation Structure**: "Your PBC structure with the Long-Term Benefit Trust is unique in AI - it shows commitment to long-term societal benefit. We've designed our approach to support enterprises who share these values..."

2. **Scaling with Mission Alignment**: "Your challenge of maintaining mission alignment while scaling to 300,000+ customers is exactly what we're helping [vertical] enterprises solve through..."

3. **Strategic Partnership Philosophy**: "The Google and Amazon partnerships show strategic thinking about infrastructure and reach. We see ourselves as a complementary partner that enhances Claude's value in [vertical]..."

4. **Culture and Values**: "Building mission-aligned teams at scale - the focus of your Stanford ETL talk - resonates with how we think about..."

**Topics to Emphasize**:
- ✅ Mission alignment and values
- ✅ Responsible scaling and governance
- ✅ Long-term partnership potential
- ✅ Enterprise customer success
- ✅ Cultural and organizational fit
- ✅ Sustainable value creation

**Topics to Avoid**:
- ❌ Short-term profit maximization
- ❌ Pure transactional relationships
- ❌ Ignoring governance and compliance
- ❌ Misalignment with PBC mission
- ❌ Pressure tactics or aggressive sales

**Customized Value Proposition** (for this person):

"We help Anthropic's enterprise customers in [vertical] adopt Claude with the governance frameworks and compliance mechanisms your Responsible Scaling Policy requires. This accelerates your enterprise expansion while maintaining the mission alignment and values that define Anthropic as a Public Benefit Corporation. Our partnership model is designed for long-term collaboration, not transactional relationships."

**Credibility Builders**:
- Reference their work: "Your work building teams at Anthropic and OpenAI shows deep understanding of what it takes to scale mission-driven AI organizations"
- Align with philosophy: "We share your commitment to purpose over pure profit"
- Demonstrate homework: "Understanding that you oversee the Long-Term Benefit Trust, we've designed with generational thinking, not quarterly metrics"

**Rapport Building Opportunities**:
- Shared values: Mission-driven work, effective altruism connections
- Operational excellence: If you have scaling experience at high-growth companies
- Strategic partnerships: Demonstrate partnership thinking, not vendor mentality

**Ideal Opening Line** (when meeting):

"Daniela, your work scaling Anthropic while maintaining its mission as a Public Benefit Corporation is remarkable. We're helping [vertical] enterprises adopt Claude with the kind of governance and compliance frameworks your Responsible Scaling Policy envisions. I'd love to explore how we can support your enterprise expansion while staying true to Anthropic's values."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"How does this align with our Public Benefit Corporation mission?"**:
   - How to handle: Lead with societal benefit, not just commercial value. Show long-term thinking.

2. **"We're very careful about partnerships - what's your culture like?"**:
   - How to handle: Demonstrate cultural fit, mission alignment, and values-driven approach.

**Competitive Relationships**:
- Close connections to effective altruism community (married to Holden Karnofsky)
- Left OpenAI for values alignment reasons - sensitive about culture/mission fit

**Timing Considerations**:
- Currently focused on global expansion and team scaling
- Recent massive fundraising creates partnership opportunities
- Best timing: When you can demonstrate enterprise customer success and mission alignment

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/daniela-amodei-790bb22a/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/DanielaAmodei) - Accessed 2025-01-15
3. [Stanford ETL Event](https://ecorner.stanford.edu/event/feb-14-2024-etl-with-daniela-amodei/) - Feb 2024
4. [Vanta Frameworks for Growth](https://www.vanta.com/resources/hiring-10x-engineers-at-anthropic) - 2025
5. [Notion First Block Interview](https://www.notion.com/blog/first-block-with-daniela-amodei) - Date not specified
6. Wikipedia entry on Daniela Amodei - Accessed 2025-01-15

**Secondary Sources**:
7. [Anthropic Engagement Brief](internal) - January 2025
8. Forbes interview coverage - March 2024
9. Sequoia AI Ascent 2024 Conference - Speaker list
10. Various LinkedIn posts about Anthropic milestones

---

## Profile: Jared Kaplan - Chief Science Officer & Responsible Scaling Officer

**Quick Summary**: Theoretical physicist from Johns Hopkins who pioneered scaling laws research, now leads Anthropic's scientific direction and ensures Responsible Scaling Policy compliance as company's dedicated safety officer.

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Jared Kaplan |
| **Title** | Chief Science Officer & Responsible Scaling Officer (Co-Founder) |
| **Department** | Research & Safety |
| **Reports To** | CEO Dario Amodei |
| **Tenure** | Co-founded Anthropic in 2021 (4 years); on leave from Johns Hopkins through '24/'25 |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/jared-kaplan-645843213/) - 138 connections |
| **Email** | Inferred format: jared@anthropic.com |
| **Twitter/X** | [@JaredKaplan](https://x.com/JaredKaplan) |

---

### Background & Career Trajectory

**Education**:
- PhD in Theoretical Physics from Harvard University
- 15 years as theoretical physics researcher before transitioning to AI

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - Chief Science Officer & Responsible Scaling Officer (2021 - Present)
   - Key Achievements:
     - Developed and enforces Responsible Scaling Policy (RSP)
     - Oversees AI Safety Levels (ASL) framework implementation
     - Leads Constitutional AI research direction
     - Pioneered mechanistic interpretability research ("brain scan for AI")
     - Anonymous reporting channel for staff safety concerns

2. **Johns Hopkins University** (15+ years, currently on leave)
   - Theoretical Physics Faculty
   - Location: Bloomberg Hall, Johns Hopkins University
   - Notable: Deep academic research background in theoretical physics
   - Currently on leave of absence through 2024/2025 academic year

3. **OpenAI** (Before 2021)
   - AI Safety and Scaling Laws Researcher
   - Notable: Co-authored foundational scaling laws research
   - Relevant: His research on scaling laws "revolutionized the AI industry by providing a framework for understanding and predicting the behavior of advanced AI systems"
   - Helped develop GPT-3 and Codex

**Domain Expertise**: Scaling Laws, AI Safety, Constitutional AI, Mechanistic Interpretability, Theoretical Physics, Responsible Scaling

**Industry Experience**: AI Research, AI Safety, Academic Research, Frontier Model Development

---

### Published Perspectives & Thought Leadership

**Recent Interviews** (Last 12 months):

1. **"Anthropic's Jared Kaplan Charts the Course for Enterprise AI" - TechCrunch Sessions: AI**
   - Date: June 5, 2025
   - URL: [TechCrunch](https://techcrunch.com/video/anthropics-jared-kaplan-charts-the-course-for-enterprise-ai/)
   - Key Quote: Discussed "hybrid reasoning models and shared insights into Anthropic's risk-governance framework for mitigating potential AI risks"
   - Topics: Enterprise AI, risk governance, AI safety frameworks, human-computer interaction transformation

2. **"Claude 3.7 Reducing Tasks from Weeks to Minutes" - The Information**
   - Date: 2025
   - Key Quote: "Claude 3.7 is getting great feedback in finance, law, & biotech, reducing tasks from weeks to minutes"
   - Topics: AI adoption in knowledge work, enterprise productivity

3. **"AI Can Build Bespoke Software On Demand" - VentureBeat**
   - Date: August 2025
   - Key Quote: "AI doesn't just use pre-built software but crafts bespoke software on demand - AI can build the software you need for a given situation in collaboration with you"
   - Topics: Future of software development, AI-powered customization

**Conference Presentations**:

- **TechCrunch Sessions: AI** (June 5, 2025): UC Berkeley Zellerbach Hall - Hybrid reasoning models, risk governance
- **Salesforce TrailblazerDX 2024**: Conversation with Clara Shih on future of AI
- **ICTS Colloquium** (April 23, 2024): "Human-Level AI by 2030?" - Summarized rapid AI progress and expectations
- **Y Combinator AI Startup School** (San Francisco): "Scaling and the Road to Human-Level AI"

**Research Publications**:
- Google Scholar profile shows extensive AI research publications
- OpenReview profile active with peer-reviewed AI safety papers

**Social Media Themes**:
- Frequently discusses: Scaling laws, mechanistic interpretability, responsible AI development, enterprise AI adoption
- Engagement triggers: Safety research breakthroughs, interpretability advances, policy frameworks

---

### Stated Priorities & Strategic Focus

Based on analysis of: conference talks, interviews, research publications, role as RSO

**Priority 1**: Responsible Scaling Policy (RSP) Implementation and Enforcement
- **Evidence**: Holds dedicated "Responsible Scaling Officer" role - unique position in AI industry
- **Source**: Company structure and Anthropic RSP documentation
- **Your Alignment**: Any solution must demonstrate RSP compliance and safety-first approach

**Priority 2**: Mechanistic Interpretability - "Brain Scan for AI"
- **Evidence**: "Explaining mechanistic interpretability research and dictionary learning, comparing it to a 'brain scan for AI', teases upcoming research on interpretable reasoning 'circuits'"
- **Source**: [X Post by Gill Verdon](https://x.com/GillVerd/status/1899244944511889886)
- **Your Alignment**: Solutions providing interpretability and transparency into AI decision-making

**Priority 3**: Scaling Research to Human-Level AI
- **Evidence**: Talk titled "Human-Level AI by 2030?" and Y Combinator presentation on "Scaling and the Road to Human-Level AI"
- **Source**: ICTS Colloquium (April 2024), YC AI Startup School
- **Your Alignment**: Understanding scaling dynamics and safety at increasing capability levels

**Current Initiatives Leading**:
- AI Safety Levels (ASL) framework evolution and enforcement
- Mechanistic interpretability research program
- Anonymous staff reporting system for RSP concerns
- Research direction for Constitutional AI advancement
- Enterprise AI safety frameworks for regulated industries

**Expressed Pain Points**:
- "How do you operationalize the Responsible Scaling Policy at frontier model scale?" - Challenge of implementing safety at scale
- "What safety level transitions have been most challenging?" - Managing ASL transitions
- Balancing research publication with competitive considerations
- Understanding why AI makes certain choices (interpretability gap)

**Success Metrics They Care About**:
- ASL framework compliance before capability deployment
- Interpretability research breakthroughs
- Safety measure effectiveness at scale
- Research publication quality and peer review
- Zero safety incidents during scaling

---

### Communication Style Analysis

**Tone & Approach**: Technical, rigorous, safety-focused, research-oriented
- Evidence: Academic background shows in careful, evidence-based presentations
- Talks combine technical depth with accessible explanations

**Preferred Language**:
- Highly technical when discussing scaling laws and interpretability
- Can translate complex research for enterprise audiences
- Emphasizes evidence and peer-reviewed research
- Values intellectual rigor and acknowledging unknowns

**Engagement Patterns** (from social media/events):
- Responds most to: Safety research, scaling law discussions, interpretability breakthroughs
- Shares content about: AI safety frameworks, research advances, responsible development
- Avoids: Hype, unfounded capability claims, ignoring safety considerations

**Communication Channels**:
- **Preferred**: Academic conferences, technical presentations, research publications
- **Activity Level**: Active at technical conferences, selective social media presence
- **Academic Website**: [Johns Hopkins](https://sites.krieger.jhu.edu/jared-kaplan/)

---

### Influence & Decision Authority

**Decision Authority**: Strong Influence + Safety Veto Power
- As RSO, can block model deployment if safety criteria not met
- Not final budget approver, but shapes research direction and safety requirements

**Budget Control**: Influences research budget allocation, but shared decision-making

**Political Capital**: Very High
- Pioneered scaling laws that revolutionized AI industry
- Holds unique "Responsible Scaling Officer" position
- Trusted to enforce safety policies even against commercial pressure
- Academic credibility from Johns Hopkins faculty position

**Innovation Profile**: Innovator (Research) + Pragmatist (Safety)
- Evidence: Pioneered scaling laws research, but pragmatic about safety enforcement
- Philosophy: "Advance capabilities responsibly with proven safety measures"

**Alignment with Your Offering**: Strong (if demonstrates safety/interpretability) / Weak (if ignores RSP framework)
- Reasoning: Will champion solutions that enhance safety, interpretability, or responsible deployment

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with technical credibility and demonstrate deep understanding of RSP framework and interpretability requirements. Jared values rigorous, research-backed approaches with clear safety properties. Show you understand scaling laws and how safety must scale with capabilities. Reference mechanistic interpretability research. Be prepared for technical depth and evidence-based discussion. Connect to enterprise safety requirements in regulated industries. Acknowledge tradeoffs honestly.

**Conversation Hooks** (Reference These):

1. **Responsible Scaling Policy**: "Your role as Responsible Scaling Officer is unique in the industry - creating actual accountability for safety commitments. Our solution helps enterprises implement ASL-aligned controls when deploying Claude in [vertical]..."

2. **Mechanistic Interpretability**: "Your work on mechanistic interpretability - the 'brain scan for AI' - addresses the critical gap in understanding AI decision-making. We're building on these principles to provide interpretability for [specific use case]..."

3. **Scaling Laws**: "Your scaling laws research fundamentally changed how we think about AI development. We've applied similar thinking to [domain], understanding how [metric] scales with [parameter]..."

4. **Enterprise AI Safety**: "You mentioned Claude 3.7 reducing tasks from weeks to minutes in finance and law - we're helping those enterprises do so with the safety frameworks your RSP envisions..."

**Topics to Emphasize**:
- ✅ RSP framework compliance and implementation
- ✅ Mechanistic interpretability and transparency
- ✅ Scaling laws and safety at scale
- ✅ Research-backed methodology
- ✅ Enterprise safety requirements
- ✅ Honest acknowledgment of limitations

**Topics to Avoid**:
- ❌ Capability without safety consideration
- ❌ Unfounded claims without evidence
- ❌ Ignoring interpretability requirements
- ❌ Rushing deployment without validation
- ❌ Black-box approaches

**Customized Value Proposition** (for this person):

"We've built [solution] with RSP-aligned safety properties that scale as capabilities increase. Our mechanistic interpretability approach provides the 'brain scan' visibility your research emphasizes, enabling enterprises in [vertical] to deploy Claude with full transparency into decision-making processes. This is backed by [research/evidence], maintaining the intellectual rigor you value."

**Credibility Builders**:
- Reference their research: "Your scaling laws paper provided the foundation for understanding how [our approach] will behave at scale"
- Demonstrate technical depth: Show you understand ASL framework, interpretability challenges, scaling dynamics
- Academic rigor: Cite research, acknowledge limitations, show peer-reviewed thinking

**Rapport Building Opportunities**:
- Academic background: If you have physics/theory background
- Research orientation: Peer-reviewed publications, academic rigor
- Safety focus: Demonstrated commitment to responsible development

**Ideal Opening Line** (when meeting):

"Jared, your work on scaling laws and mechanistic interpretability has been foundational for our field. As Responsible Scaling Officer, you're uniquely positioned to ensure safety scales with capability. We're helping [vertical] enterprises deploy Claude with the interpretability and safety controls your RSP framework requires, backed by [evidence]. I'd value your technical perspective on [specific challenge]."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"How does this maintain safety properties as capabilities scale?"**:
   - How to handle: Show you've thought about scaling dynamics, provide evidence of safety at different scales

2. **"Can you demonstrate mechanistic interpretability of your approach?"**:
   - How to handle: Provide technical details on how your system's decisions can be understood and audited

**Competitive Relationships**:
- Academic integrity is paramount - don't misrepresent research
- RSO role means safety cannot be compromised for commercial gain

**Timing Considerations**:
- Currently focused on ASL framework evolution and interpretability research
- Best timing: When you have technical evidence and research backing

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/jared-kaplan-645843213/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/JaredKaplan) - Accessed 2025-01-15
3. [Johns Hopkins Faculty Page](https://sites.krieger.jhu.edu/jared-kaplan/) - Accessed 2025-01-15
4. [TechCrunch Sessions: AI](https://techcrunch.com/2025/05/12/anthropic-co-founder-jared-kaplan-is-coming-to-techcrunch-sessions-ai/) - June 2025
5. [VentureBeat Interview](https://venturebeat.com/ai/bespoke-software-on-demand-anthropics-ai-powered-future) - Aug 2025
6. [ICTS Colloquium](https://www.icts.res.in/colloquium/2024-04-23/jared-kaplan) - April 2024
7. Google Scholar profile - Accessed 2025-01-15
8. OpenReview profile - Accessed 2025-01-15

**Secondary Sources**:
9. [Anthropic Engagement Brief](internal) - January 2025
10. Y Combinator content on Jared Kaplan
11. Various X posts about his mechanistic interpretability work

---

## Profile: Jack Clark - Co-Founder, Head of Policy

**Quick Summary**: Former technology journalist and OpenAI Policy Director who co-founded Anthropic to advance responsible AI policy. Writes Import AI newsletter (70,000 subscribers) and serves on NAIAC and OECD committees.

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Jack Clark |
| **Title** | Co-Founder, Head of Policy |
| **Department** | Policy & Strategic Initiatives |
| **Reports To** | Board of Directors (Board Member) |
| **Tenure** | Co-founded Anthropic in 2021 (4 years) |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/jack-clark-5a320317/) |
| **Email** | Inferred format: jack@anthropic.com |
| **Twitter/X** | [@jackclarkSF](https://x.com/jackclarkSF) - 97.8K followers |

---

### Background & Career Trajectory

**Education**:
- Technology journalism background
- Deep self-directed learning in AI policy and governance

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - Co-Founder & Head of Policy (2021 - Present)
   - Board Member
   - Key Achievements:
     - Led Anthropic's endorsement of California SB 53 (frontier AI transparency)
     - Testified before Congress on AI safety (Feb 2024, June 2025)
     - Established Frontier Red Team for threat modeling
     - Represents Anthropic in policy forums globally
     - ONEAI OECD co-chair

2. **OpenAI** (Before 2021)
   - Policy Director
   - Notable: Shaped OpenAI's early policy positions
   - Left to co-found Anthropic with different approach to AI safety

3. **Technology Journalism** (Before OpenAI)
   - Technical journalist writing about distributed systems, quantum computers, and AI research
   - Publications: Bloomberg BusinessWeek, The Register
   - Relevant: Deep understanding of communicating complex technical topics

**Other Key Roles**:
- Founding member of AI Index at Stanford University (2017-2024)
- Inaugural member of USA's National Artificial Intelligence Advisory Committee (NAIAC) (2021-2024)
- Advisory councils: CNAS, OECD
- ONEAI OECD co-chair (current)

**Domain Expertise**: AI Policy, Government Relations, Strategic Communications, AI Governance, Regulatory Frameworks, Threat Modeling

**Industry Experience**: AI Research Organizations, Technology Journalism, Policy Advisory Bodies, Government Relations

---

### Published Perspectives & Thought Leadership

**Personal Newsletter**:

1. **Import AI** - Weekly Newsletter
   - URL: [jack-clark.net](https://jack-clark.net/) and [importai.substack.com](https://importai.substack.com/)
   - Readership: 70,000 subscribers
   - Content: AI research analysis, policy discussions, science fiction stories about AI
   - Frequency: Weekly
   - Notable: One of most influential AI policy newsletters

**Recent Essays & Posts**:

1. **"Technological Optimism and Appropriate Fear"**
   - Date: October 2025
   - Key Argument: Discusses AI risks and need to acknowledge different realities before managing the technology
   - Topics: AI existential risk, balanced perspective on progress and danger

**Congressional Testimony**:

1. **House Committee on Science, Space and Technology**
   - Date: February 6, 2024 and June 25, 2025
   - URL: [Testimony PDFs available](https://www.congress.gov/119/meeting/house/118428/witnesses/HHRG-119-ZS00-Wstate-ClarkJ-20250625.pdf)
   - Topics: AI safety evaluations, US vs China AI capabilities, national security implications

**Conference Presentations**:

- **Johns Hopkins University** (March 2025): "AI & Safety" talk for Hopkins Software Engineering Club
- **Conversations with Tyler Cowen**: Discussed AI's economic impact and Anthropic Economic Index (March 2025)
- **OECD AI-WIPS Conference** (2022): Speaker on AI policy

**Recent Public Statements** (X/Twitter):

1. **California SB 53 Endorsement**
   - Date: Multiple posts 2025
   - Key Quote: "Anthropic is endorsing SB 53, California Sen. @Scott_Wiener's bill requiring transparency of frontier AI companies. We have long said we would prefer a federal standard. But in the absence of that this creates a solid blueprint for AI governance that cannot be ignored."
   - Also: "We applaud @CAgovernor for signing @Scott_Wiener's SB 53"

2. **Frontier Red Team**
   - Date: December 2024
   - Key Quote: "Fun story about the Frontier Red Team at Anthropic. I expect coming up with better and more realistic threat models for frontier risks is going to be one of the more important areas of AI policy to work on in 2025."

3. **AI for Science Program**
   - Key Quote: "Are you trying to use AI to accelerate science? Do you work for a nonprofit or academic institution? Would you like up to $20,000 of free API credits for @AnthropicAI? Please fill out this form"

**Social Media Themes**:
- Frequently discusses: AI policy, frontier risks, threat modeling, regulatory frameworks
- Engagement triggers: Policy developments, safety research, government AI initiatives
- Active voice on AI governance debates

---

### Stated Priorities & Strategic Focus

Based on analysis of: congressional testimony, Import AI newsletter, X posts, policy statements

**Priority 1**: AI Policy and Regulatory Frameworks
- **Evidence**: Led Anthropic's endorsement of California SB 53; testified to Congress twice; NAIAC member
- **Source**: Multiple policy engagements and public statements
- **Your Alignment**: Demonstrate compliance with emerging regulatory requirements

**Priority 2**: Frontier Risk Assessment and Threat Modeling
- **Evidence**: "Coming up with better and more realistic threat models for frontier risks is going to be one of the more important areas of AI policy to work on in 2025"
- **Source**: [X Post Dec 2024](https://x.com/jackclarkSF/status/1866778044947398876)
- **Your Alignment**: Show how you address or mitigate frontier risks

**Priority 3**: Transparency and Public Communication
- **Evidence**: Writes Import AI for 70,000 readers; advocates for transparency requirements in SB 53
- **Source**: Import AI newsletter, SB 53 endorsement
- **Your Alignment**: Emphasize transparency, explainability, and clear communication

**Current Initiatives Leading**:
- Frontier Red Team development for realistic threat modeling
- Congressional engagement and testimony on AI safety
- OECD AI governance participation as ONEAI co-chair
- California AI legislation support (SB 53)
- Import AI newsletter and public education

**Expressed Pain Points**:
- "How do you see AI regulation evolving in the next 18 months?" - Regulatory uncertainty
- "What policy frameworks would best support responsible AI deployment?" - Need for clear frameworks
- Fragmented regulatory landscape (prefers federal standard, settles for state-level)
- Gap between AI capabilities and policy understanding

**Success Metrics They Care About**:
- Quality of regulatory frameworks adopted
- Industry adoption of safety standards
- Public understanding of AI risks and benefits
- Threat model accuracy and realism
- Government AI policy sophistication

---

### Communication Style Analysis

**Tone & Approach**: Journalistic, clear, policy-focused, balanced between optimism and caution
- Evidence: Import AI newsletter combines technical analysis with accessible writing
- Former journalist background shows in communication clarity

**Preferred Language**:
- Clear, jargon-free explanations for policy audiences
- Can translate between technical and policy domains
- Values storytelling (includes sci-fi in Import AI)
- Balanced perspective acknowledging both benefits and risks

**Engagement Patterns** (from social media/events):
- Responds most to: Policy developments, threat modeling discussions, regulatory initiatives
- Shares content about: AI governance, safety research, policy frameworks, Import AI posts
- Avoids: Partisan politics, pure technical details without policy implications

**Communication Channels**:
- **Preferred**: Congressional testimony, policy forums, written analysis (Import AI)
- **Activity Level**: Very active on X/Twitter (97.8K followers), weekly Import AI newsletter
- **Personal Website**: [jack-clark.net](https://jack-clark.net/)

---

### Influence & Decision Authority

**Decision Authority**: Strong Influence (Board Member) - Shapes policy and strategic direction, not operational budget

**Budget Control**: Influences strategic initiatives budget, especially policy-related spending

**Political Capital**: Extremely High
- NAIAC member (2021-2024) - direct government advisory role
- Congressional testimony (multiple times)
- OECD ONEAI co-chair
- 97.8K Twitter followers - influential voice in AI policy
- Import AI reaches 70,000 readers weekly

**Innovation Profile**: Early Adopter (Policy) / Pragmatist (Implementation)
- Evidence: Pushes for advanced policy frameworks but pragmatic about implementation
- Philosophy: "Prefer federal standards, but support state-level action in their absence"

**Alignment with Your Offering**: Strong (if policy-compliant and transparent) / Weak (if regulatory risk)
- Reasoning: Will champion solutions that advance responsible AI deployment with proper governance

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with policy awareness and demonstrate understanding of regulatory landscape. Jack values clear communication about how your solution fits within emerging AI governance frameworks. Show you've read Import AI and understand frontier risks. Reference specific policy developments he's championed (like SB 53). Demonstrate transparency and willingness to engage with governance requirements. Connect to broader AI safety ecosystem. Be prepared to discuss implications for government, not just enterprise.

**Conversation Hooks** (Reference These):

1. **California SB 53 and Transparency**: "Your endorsement of SB 53 shows Anthropic's commitment to transparency even without federal mandates. Our solution helps enterprises implement the kind of transparency requirements SB 53 envisions..."

2. **Frontier Red Team and Threat Modeling**: "I saw your post about the Frontier Red Team - better threat models for frontier risks. We're working on [specific threat model] for [vertical] deployment, recognizing..."

3. **Import AI Newsletter**: "I've been reading Import AI for [time period] - your analysis of [recent topic] was particularly insightful. It connects to what we're building in..."

4. **Policy-Technical Translation**: "Your background translating between technical AI and policy audiences is unique. We're solving a similar translation problem for [vertical] enterprises needing to understand..."

**Topics to Emphasize**:
- ✅ Regulatory compliance and governance
- ✅ Transparency and explainability
- ✅ Frontier risk awareness
- ✅ Policy implications of deployment
- ✅ Government and public benefit
- ✅ Clear communication to non-technical stakeholders

**Topics to Avoid**:
- ❌ Ignoring regulatory requirements
- ❌ Black-box approaches without transparency
- ❌ Dismissing policy concerns as "red tape"
- ❌ Purely commercial focus without societal consideration
- ❌ Attacking or undermining regulatory efforts

**Customized Value Proposition** (for this person):

"We've designed [solution] to meet the transparency and governance requirements that regulations like SB 53 establish - exactly the kind of responsible AI deployment framework you've championed. By helping [vertical] enterprises deploy Claude with proper oversight and explainability, we're advancing the policy goals you've articulated in Import AI and congressional testimony, while enabling practical AI adoption."

**Credibility Builders**:
- Reference Import AI: "Your recent Import AI issue on [topic] highlighted the exact challenge we're addressing"
- Policy awareness: Demonstrate you follow AI governance developments
- Balanced perspective: Show you understand both benefits and risks, like his "Technological Optimism and Appropriate Fear" essay

**Rapport Building Opportunities**:
- Import AI readership: Genuine engagement with his newsletter
- Policy interest: If you've worked on government relations or regulatory compliance
- Communication skills: Shared interest in translating complex topics

**Ideal Opening Line** (when meeting):

"Jack, I've been reading Import AI since [when] - your analysis of frontier risks and policy frameworks has shaped how we think about responsible deployment. Anthropic's support for SB 53 shows real commitment to transparency. We're helping [vertical] enterprises implement the kind of governance and oversight that policy frameworks like SB 53 require, while making Claude practical for [use case]."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"How does this align with emerging regulatory requirements?"**:
   - How to handle: Show detailed understanding of SB 53, EU AI Act, or relevant regulations. Demonstrate compliance design.

2. **"What are the frontier risks of this approach?"**:
   - How to handle: Show you've thought about threat models. Be honest about risks and mitigation strategies.

**Competitive Relationships**:
- Journalist background means values accuracy and fact-checking
- Left OpenAI for Anthropic - sensitive about mission/values differences

**Timing Considerations**:
- Currently focused on 2025 frontier risk threat modeling
- Post-SB 53 signing - good time for governance-focused solutions
- Best timing: When you can demonstrate policy compliance and transparency

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/jack-clark-5a320317/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/jackclarkSF) - Accessed 2025-01-15
3. [Import AI Website](https://jack-clark.net/) - Accessed 2025-01-15
4. [Import AI Substack](https://importai.substack.com/) - Accessed 2025-01-15
5. [Congressional Testimony June 2025](https://www.congress.gov/119/meeting/house/118428/witnesses/HHRG-119-ZS00-Wstate-ClarkJ-20250625.pdf)
6. [Congressional Testimony Feb 2024](https://democrats-science.house.gov/download/jack-clark_-testimony)
7. [Johns Hopkins Event](https://hub.jhu.edu/events/2025/03/27/jack-clark-co-founder-of-anthropic-ai-safety/) - March 2025
8. Conversations with Tyler Cowen appearance - 2025

**Secondary Sources**:
9. [Anthropic Engagement Brief](internal) - January 2025
10. Multiple X posts on SB 53, Frontier Red Team, AI for Science
11. Stanford AI Index founding member documentation

---

## Profile: Tom Brown - Co-Founder, Chief Compute Officer

**Quick Summary**: Self-taught engineer who overcame "B-minus in linear algebra" to co-author GPT-3 paper at OpenAI, now leads compute infrastructure and API engineering at Anthropic. Champions quality over benchmark gaming.

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Tom Brown |
| **Title** | Co-Founder, Chief Compute Officer (leading API Engineering) |
| **Department** | Technical Infrastructure & Compute |
| **Reports To** | CEO Dario Amodei |
| **Tenure** | Co-founded Anthropic in 2021 (4 years) |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/nottombrown/) |
| **Email** | tom@anthropic.com (confirmed from public post) |
| **Twitter/X** | [@nottombrown](https://x.com/nottombrown) |

---

### Background & Career Trajectory

**Education**:
- Massachusetts Institute of Technology (MIT)
- Self-taught in many AI/ML areas (overcame weak linear algebra background)

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - Co-Founder & Chief Compute Officer (2021 - Present)
   - Leading API Engineering
   - Key Achievements:
     - Built API platform serving 300,000+ business customers
     - Led compute infrastructure optimization (AWS/Annapurna Labs collaboration)
     - Championed anti-benchmark-gaming philosophy
     - Scaled infrastructure from founding to $5B revenue run-rate
     - Optimizing Trainium silicon-to-software stack

2. **OpenAI** (Before 2021)
   - Member of Technical Staff
   - Led engineering of GPT-3
   - Co-authored seminal GPT-3 paper "Language Models are Unsupervised Multitask Learners"
   - Notable: From research to engineering leadership on GPT-3
   - Left with Dario, Daniela, and team to co-found Anthropic

3. **Google Brain** (Before OpenAI)
   - Member of Technical Staff
   - Early AI research and engineering experience

4. **Y Combinator Founder** (Earlier career)
   - Startup founder before transitioning to AI

**Domain Expertise**: Compute Infrastructure, API Engineering, Large-Scale Training, Silicon Optimization, Distributed Systems

**Industry Experience**: AI Infrastructure, Startup Founding, Large-Scale ML Systems, Cloud Computing

---

### Published Perspectives & Thought Leadership

**Recent Interviews** (Last 12 months):

1. **Y Combinator Lightcone Podcast** - "AI Scaling Breakthroughs"
   - Date: 2025
   - Key Quote: Discussed journey from "getting a B-minus in linear algebra to becoming one of the key people behind AI's scaling breakthroughs"
   - Topics: Self-taught engineering, AI infrastructure buildout, career advice

2. **"In Conversation With Tom Brown" - Salesforce Ventures**
   - Date: December 2023 (fireside chat with David Schmaier in San Francisco)
   - URL: [Salesforce Ventures](https://salesforceventures.com/perspectives/in-conversation-with-anthropic-co-founder-tom-brown/)
   - Topics: Reducing hallucinations, improving LLM output quality, AI use cases

3. **Business Insider - "5 Career Tips"**
   - Date: August 2025
   - Key Message: Networked and self-studied way into AI
   - Topics: Career development, self-teaching, breaking into AI

**Conference Presentations**:

- **AWS re:Invent 2024**: Customer speaker
- **Y Combinator Events**: Multiple appearances on scaling and infrastructure

**Key Public Statements** (X/Twitter):

1. **Anti-Benchmark Gaming Philosophy**
   - Key Quote: "We don't teach to the benchmark. I think the benchmarks are easy to game. All the other big labs have teams whose whole job is to make the benchmark scores good. We don't have such a team. That is the biggest factor [in our coding performance]."
   - Source: Multiple X posts
   - Philosophy: Quality over artificial metrics

2. **AWS/Annapurna Labs Collaboration**
   - Date: November 2024
   - Key Quote: "Excited to get to work with AWS and Annapurna Labs on optimizing Trainium from silicon to software. Our team's been having fun going deep into the Neuron stack to get as close as possible to 100% peak theoretical performance."
   - Topics: Compute optimization, infrastructure partnership

3. **Hiring for Compute Efficiency**
   - Key Quote: "Now seems like a good time to mention that we're always looking for ways to more efficiently turn raw compute into useful safety research. If you know of great software engineers who are interested in building big machines then have them message me at tom@anthropic.com"
   - Topics: Recruiting, infrastructure focus

**Social Media Themes**:
- Frequently discusses: Compute infrastructure, engineering efficiency, quality over metrics
- Engagement triggers: Hardware optimization, engineering challenges, anti-hype messaging
- Values: Authenticity, technical depth, avoiding benchmark gaming

---

### Stated Priorities & Strategic Focus

Based on analysis of: X posts, interviews, conference presentations, role as Chief Compute Officer

**Priority 1**: Compute Efficiency and Infrastructure Optimization
- **Evidence**: "Our team's been having fun going deep into the Neuron stack to get as close as possible to 100% peak theoretical performance"
- **Source**: [X Post Nov 2024](https://x.com/nottombrown/status/1859972774158348689)
- **Your Alignment**: Solutions that improve compute efficiency or infrastructure utilization

**Priority 2**: Quality Over Benchmark Gaming
- **Evidence**: "All the other big AI labs have teams whose job it is to make the benchmark scores good. We don't have such a team. That is the biggest factor."
- **Source**: Multiple X posts and interviews
- **Your Alignment**: Real-world effectiveness over artificial metrics

**Priority 3**: API Platform Excellence for Enterprise
- **Evidence**: Leading API Engineering role; serves 300,000+ business customers
- **Source**: Role description and company metrics
- **Your Alignment**: Enterprise-grade reliability, performance, and developer experience

**Current Initiatives Leading**:
- AWS Trainium optimization ("silicon to software")
- API platform scaling for enterprise growth
- Compute infrastructure for frontier model training
- Engineering team building ("always looking for software engineers who are interested in building big machines")

**Expressed Pain Points**:
- "More efficiently turn raw compute into useful safety research" - Compute efficiency challenge
- Benchmark gaming creating false signals about capability
- Scaling infrastructure while maintaining quality

**Success Metrics They Care About**:
- Compute utilization (approaching 100% peak theoretical performance)
- Real-world API reliability and performance
- Engineering efficiency (doing more with less)
- Quality metrics over vanity benchmarks
- Developer experience and satisfaction

---

### Communication Style Analysis

**Tone & Approach**: Pragmatic, engineering-focused, anti-hype, authentic
- Evidence: Direct communication style, admits past struggles ("B-minus in linear algebra")
- Self-taught background shows in practical, results-oriented thinking

**Preferred Language**:
- Technical but accessible
- Values honesty over polish (admits weaknesses, avoids marketing speak)
- Engineering mindset: efficiency, optimization, real-world performance
- Skeptical of hype and artificial metrics

**Engagement Patterns** (from social media/events):
- Responds most to: Infrastructure challenges, engineering problems, compute optimization
- Shares content about: AWS collaboration, team achievements, hiring for hard problems
- Avoids: Marketing hype, benchmark bragging, unfounded capability claims

**Communication Channels**:
- **Preferred**: Technical discussions with engineers, infrastructure deep-dives
- **Activity Level**: Active on X/Twitter, technical conference speaker
- **Email**: tom@anthropic.com (publicly shared for engineer recruiting)

---

### Influence & Decision Authority

**Decision Authority**: Strong Influence on technical infrastructure decisions
- Not final budget approver, but shapes compute and API strategy

**Budget Control**: Influences infrastructure and compute budget (significant given training costs)

**Political Capital**: High
- Co-founder status
- Author of GPT-3 (landmark paper)
- Respected for anti-benchmark-gaming stance
- Deep technical credibility

**Innovation Profile**: Innovator (Infrastructure) / Pragmatist (Deployment)
- Evidence: Pushing boundaries on compute optimization, but focused on real-world results
- Philosophy: "Build what works, not what benchmarks well"

**Alignment with Your Offering**: Strong (if infrastructure-related or enterprise-focused) / Moderate (if not technical)
- Reasoning: Will champion solutions that improve infrastructure efficiency or API platform quality

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with technical credibility and real-world results, not benchmarks or marketing hype. Tom values engineering honesty and practical effectiveness. Show you understand compute infrastructure challenges and care about efficiency. Reference his anti-benchmark-gaming philosophy and demonstrate you share that value. Connect to API platform needs or infrastructure optimization. Be direct and authentic - avoid sales polish. If recruiting engineers, he's publicly receptive (tom@anthropic.com).

**Conversation Hooks** (Reference These):

1. **Anti-Benchmark Gaming**: "I love your stance on benchmark gaming - 'we don't have a team whose job is to make the benchmark scores good.' We've taken the same approach with [solution], focusing on real-world [metric] instead of..."

2. **Compute Optimization**: "Your work with AWS on Trainium optimization - getting to 100% peak theoretical performance - is exactly the kind of efficiency challenge we're tackling in [domain]..."

3. **Self-Taught Engineering Journey**: "Your path from B-minus in linear algebra to co-authoring the GPT-3 paper is inspiring. I've had a similar journey in [domain], learning..."

4. **API Platform at Scale**: "Leading API engineering for 300,000+ business customers is a massive infrastructure challenge. We're helping those enterprises [use case] with..."

**Topics to Emphasize**:
- ✅ Compute efficiency and infrastructure optimization
- ✅ Real-world performance over benchmarks
- ✅ Engineering challenges and technical depth
- ✅ API platform reliability and scale
- ✅ Practical effectiveness
- ✅ Honest assessment of limitations

**Topics to Avoid**:
- ❌ Benchmark bragging without real-world validation
- ❌ Marketing hype over substance
- ❌ Ignoring engineering tradeoffs
- ❌ Sales polish over technical honesty
- ❌ Dismissing infrastructure challenges

**Customized Value Proposition** (for this person):

"We've built [solution] with the same anti-benchmark-gaming philosophy you champion - optimizing for real-world [metric] that enterprises actually care about. Our approach improves [compute efficiency / API performance / infrastructure utilization] for Claude deployments in [vertical], helping you serve those 300,000+ business customers more effectively while getting closer to that 100% theoretical performance you're pushing toward with Trainium."

**Credibility Builders**:
- Technical depth: Show you understand infrastructure challenges, not just high-level concepts
- Honest assessment: Admit limitations like Tom admits past struggles
- Real results: Focus on actual performance, not vanity metrics

**Rapport Building Opportunities**:
- Self-taught background: If you've learned through practice
- Engineering mindset: Shared focus on building over talking
- Infrastructure challenges: Compute optimization, scaling systems

**Ideal Opening Line** (when meeting):

"Tom, your stance on benchmark gaming - that quality matters more than scores - resonates deeply. We've built [solution] the same way: no optimization for artificial metrics, just real-world effectiveness for [use case]. Given your focus on compute efficiency and API platform excellence, I'd love your engineering perspective on [specific technical challenge]."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"How does this perform in real-world scenarios, not just benchmarks?"**:
   - How to handle: Show actual usage data, customer results, honest performance assessment

2. **"What's the compute efficiency / infrastructure overhead?"**:
   - How to handle: Provide technical details on resource utilization and optimization

**Competitive Relationships**:
- Left OpenAI but authored GPT-3 - handle OpenAI references carefully
- Values engineering authenticity - don't oversell

**Timing Considerations**:
- Currently focused on AWS Trainium optimization and API platform scaling
- Best timing: When you have technical validation and real-world results

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/nottombrown/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/nottombrown) - Accessed 2025-01-15
3. [Y Combinator Podcast](https://x.com/ycombinator/status/1957815586744070653) - 2025
4. [AWS Trainium Post](https://x.com/nottombrown/status/1859972774158348689) - Nov 2024
5. [Benchmark Gaming Philosophy Posts](https://x.com/Hangsiin/status/1957856387524096231) - 2025
6. [Salesforce Ventures Interview](https://salesforceventures.com/perspectives/in-conversation-with-anthropic-co-founder-tom-brown/) - Dec 2023
7. Email: tom@anthropic.com (from public recruiting post)

**Secondary Sources**:
8. [Anthropic Engagement Brief](internal) - January 2025
9. Business Insider career tips article - Aug 2025
10. AWS re:Invent 2024 speaker list

---

## Profile: Sam McCandlish - Co-Founder, Chief Architect

**Quick Summary**: Theoretical physicist (PhD from Stanford/Berkeley) who pioneered scaling laws at OpenAI, now leads research and architecture at Anthropic. More research-focused and less publicly visible than other co-founders.

---

### Basic Information

| Attribute | Details |
|-----------|---------|
| **Full Name** | Sam McCandlish |
| **Title** | Co-Founder, Chief Architect |
| **Department** | Research & Architecture |
| **Reports To** | CEO Dario Amodei |
| **Tenure** | Co-founded Anthropic in 2021 (4 years) |
| **LinkedIn** | [Profile](https://www.linkedin.com/in/sam-mccandlish/) |
| **Email** | Verified: @anthropic.com domain (from Google Scholar) |
| **Twitter/X** | [@samsamoa](https://x.com/samsamoa) |
| **GitHub** | [samsamoa](https://github.com/samsamoa) - 7 repositories |

---

### Background & Career Trajectory

**Education**:
- PhD in Theoretical Physics from Stanford University (some sources indicate UC Berkeley)
- Research focused on quantum gravity, tensor networks, and tomography
- Strong theoretical foundation informing AI research

**Career Path**:

1. **Anthropic PBC** (2021 - Present)
   - Co-Founder & Chief Architect (2021 - Present)
   - Key Achievements:
     - Leads model architecture and research direction
     - Contributes to Constitutional AI methodology
     - Shapes Claude model family development
     - Research on scaling laws and AI safety
     - Co-author on sycophancy research paper

2. **OpenAI** (Before 2021)
   - Research Lead on AI Safety team
   - Built team to study science and scaling of ML
   - Notable: Work on scaling laws helped pave way for GPT-3
   - Relevant: Instrumental role in GPT-3 development
   - Left with Dario, Daniela, and team due to safety approach differences

**Domain Expertise**: Scaling Laws, AI Architecture, Theoretical Physics, AI Safety Research, Quantum Systems, Model Development

**Industry Experience**: AI Research, AI Safety, Frontier Model Development, Academic Research

---

### Published Perspectives & Thought Leadership

**Research Publications**:

1. **Google Scholar Profile**
   - URL: [Google Scholar](https://scholar.google.com/citations?user=gHp0pu4AAAAJ&hl=en)
   - Verified email: @anthropic.com
   - Focus: Scaling laws, AI safety, model behavior

2. **"Towards Understanding Sycophancy in Language Models"**
   - Co-authored research paper
   - Repository: [meg-tong/sycophancy-eval](https://github.com/meg-tong/sycophancy-eval)
   - Topic: Understanding and evaluating sycophantic behavior in LLMs

3. **Scaling Laws Research**
   - Foundational work at OpenAI
   - Citation: arxiv.org/pdf/1812.06162.pdf on batch size optimization
   - Referenced across AI research community

**Online Presence**:

- **LessWrong**: Active commenter as Anthropic cofounder
- **OpenReview**: [Profile](https://openreview.net/profile?id=~Sam_McCandlish1) - Researcher at Anthropic
- **GitHub**: 7 repositories available at samsamoa

**Public Announcements**:

- **May 2021 X Post**: "Building AI as a human ally" announcement of Anthropic founding
- Generally less publicly visible than other co-founders (more research-focused)

**Social Media Themes**:
- Limited public social media presence compared to other co-founders
- When posting: Focuses on AI safety, building AI as "human ally"
- Research-oriented rather than public-facing

---

### Stated Priorities & Strategic Focus

Based on analysis of: research publications, role as Chief Architect, founding mission

**Priority 1**: AI Architecture and Scaling Research
- **Evidence**: Chief Architect role; pioneered scaling laws at OpenAI that "revolutionized the AI industry"
- **Source**: Role description, scaling laws research citations
- **Your Alignment**: Solutions that understand or leverage scaling dynamics

**Priority 2**: AI Safety and Alignment Research
- **Evidence**: Research Lead on OpenAI AI Safety team; co-founded Anthropic for safety-focused approach
- **Source**: Career history, Anthropic founding narrative
- **Your Alignment**: Safety-first approaches, alignment research

**Priority 3**: Constitutional AI and Model Behavior
- **Evidence**: Contributes to Constitutional AI methodology; research on sycophancy in LLMs
- **Source**: Sycophancy research paper, Constitutional AI development
- **Your Alignment**: Understanding and shaping model behavior ethically

**Current Initiatives Leading**:
- Claude model family architecture and development
- Scaling laws research application to frontier models
- AI safety research within Constitutional AI framework
- Model behavior research (reducing sycophancy, improving alignment)

**Expressed Pain Points**:
- Understanding emergent model behaviors at scale
- Maintaining safety properties as capabilities increase
- Balancing research publication with competitive considerations

**Success Metrics They Care About**:
- Research quality and peer review
- Model architecture effectiveness and efficiency
- Safety properties at increasing scales
- Fundamental understanding of AI systems

---

### Communication Style Analysis

**Tone & Approach**: Research-focused, technical, lower public profile
- Evidence: Less public-facing than other co-founders; academic publication focus
- Theoretical physics background shows in rigorous, analytical thinking

**Preferred Language**:
- Highly technical and research-oriented
- Academic rigor and peer review standards
- Precise mathematical and conceptual frameworks

**Engagement Patterns** (limited public data):
- Focuses on: Research quality, technical depth, fundamental understanding
- Less active on social media compared to other co-founders
- Engages through academic channels (LessWrong, OpenReview, publications)

**Communication Channels**:
- **Preferred**: Research publications, academic forums, technical discussions
- **Activity Level**: Lower public profile; more behind-the-scenes research
- **Academic Presence**: Google Scholar, OpenReview profiles

---

### Influence & Decision Authority

**Decision Authority**: Strong Influence on research and architecture decisions
- As Chief Architect, shapes model development direction
- Not final budget approver, but influences research priorities

**Budget Control**: Influences research budget allocation

**Political Capital**: High (within technical community)
- Pioneered scaling laws (foundational AI research)
- Co-founder status
- Respected in AI safety research community
- Less public-facing than other co-founders, but deep technical credibility

**Innovation Profile**: Innovator (Research) / Safety-Conscious (Deployment)
- Evidence: Pioneering scaling laws research; left OpenAI for safety-focused approach
- Philosophy: Advance fundamental understanding while prioritizing safety

**Alignment with Your Offering**: Strong (if research-backed and technically rigorous) / Moderate (if purely applied)
- Reasoning: Will support solutions with strong technical foundations and safety properties

---

### Strategic Engagement Recommendations

**Best Approach**:

Lead with technical rigor and research backing. Sam values deep understanding of AI systems and fundamental research. Less interested in marketing or high-level pitches - focus on technical architecture, scaling properties, and safety implications. Reference scaling laws and model behavior research. Connect to Constitutional AI methodology. Be prepared for deep technical discussion. Acknowledge this is a research-focused leader, not primary external-facing contact.

**Conversation Hooks** (Reference These):

1. **Scaling Laws Research**: "Your scaling laws research at OpenAI provided the framework for understanding how [our approach] behaves as [parameter] increases..."

2. **Model Architecture**: "As Chief Architect, you're shaping Claude's development. We've designed [solution] with architectural considerations around [technical detail]..."

3. **AI Safety Research**: "Your transition from OpenAI to Anthropic for a safety-focused approach shows deep commitment. Our work on [safety aspect] aligns with..."

4. **Sycophancy Research**: "I saw your research on understanding sycophancy in LLMs - this connects to our work on [model behavior topic]..."

**Topics to Emphasize**:
- ✅ Technical architecture and research foundations
- ✅ Scaling properties and behaviors
- ✅ AI safety research and alignment
- ✅ Peer-reviewed methodology
- ✅ Fundamental understanding of systems
- ✅ Constitutional AI principles

**Topics to Avoid**:
- ❌ High-level marketing without technical substance
- ❌ Ignoring scaling implications
- ❌ Lack of research backing
- ❌ Purely commercial focus without safety consideration

**Customized Value Proposition** (for this person):

"We've built [solution] on research-backed principles similar to your scaling laws work - understanding how [system property] behaves as capabilities increase. Our architecture maintains safety properties at scale, aligning with Constitutional AI methodology. This is supported by [research/evidence] following the kind of rigorous approach you pioneered."

**Credibility Builders**:
- Research citations: Reference his scaling laws work or related research
- Technical depth: Show mathematical or architectural rigor
- Academic approach: Peer-reviewed thinking, acknowledging limitations

**Rapport Building Opportunities**:
- Theoretical physics background: If you have physics/math background
- Research orientation: Academic publications, rigorous methodology
- AI safety focus: Demonstrated commitment to safety research

**Ideal Opening Line** (when meeting):

"Sam, your scaling laws research fundamentally changed how we think about AI development. As Chief Architect, you're applying those insights to Claude's development. We've taken a similar approach to [domain], understanding how [property] scales with [parameter], while maintaining the safety properties your Constitutional AI work emphasizes."

---

### Red Flags & Caution Areas

**Potential Objections**:

1. **"What's the theoretical foundation for this approach?"**:
   - How to handle: Provide research backing, mathematical framework, or rigorous analysis

2. **"How does this behave as capabilities scale?"**:
   - How to handle: Show you've analyzed scaling properties, similar to his scaling laws work

**Competitive Relationships**:
- Academic integrity critical - don't misrepresent research
- Left OpenAI for safety reasons - sensitive about research vs commercialization balance

**Timing Considerations**:
- Less public-facing than other co-founders
- Best approached through technical channels or introductions from research community
- May not be primary contact for external partnerships (more internal research focus)

---

### Sources Consulted

**Primary Sources**:
1. [LinkedIn Profile](https://www.linkedin.com/in/sam-mccandlish/) - Accessed 2025-01-15
2. [X/Twitter Profile](https://x.com/samsamoa) - Accessed 2025-01-15
3. [GitHub Profile](https://github.com/samsamoa) - Accessed 2025-01-15
4. [Google Scholar](https://scholar.google.com/citations?user=gHp0pu4AAAAJ&hl=en) - Accessed 2025-01-15
5. [OpenReview Profile](https://openreview.net/profile?id=~Sam_McCandlish1) - Accessed 2025-01-15
6. [LessWrong Profile](https://www.lesswrong.com/users/sam-mccandlish) - Accessed 2025-01-15
7. Sycophancy research paper repository - Accessed 2025-01-15

**Secondary Sources**:
8. [Anthropic Engagement Brief](internal) - January 2025
9. Scaling laws citations across AI research
10. Various profiles (Crunchbase, The Org) describing role

---

## CROSS-CUTTING ANALYSIS

### Shared Priorities Across Leadership

**Universal Themes**:
1. **AI Safety First**: Every co-founder left OpenAI for safety-focused approach
2. **Constitutional AI**: Commitment to transparent, interpretable, steerable systems
3. **Long-term Thinking**: Public Benefit Corporation structure reflects generational perspective
4. **Research Rigor**: Academic backgrounds inform evidence-based decision making
5. **Mission Over Profit**: PBC status and Long-Term Benefit Trust show values alignment

### Influence Network

```
Dario (CEO) - Final decision authority, technical vision
    ├── Daniela (President) - Strategic operations, trusted operational partner, sibling
    ├── Jared (CSO/RSO) - Scientific direction, safety veto power
    ├── Jack (Policy) - Board member, policy direction, government relations
    ├── Tom (Compute) - Infrastructure and API platform
    └── Sam (Architect) - Research and model architecture

Key Relationship: Daniela married to Holden Karnofsky (Open Philanthropy board member)
External Influence: Effective Altruism community, though Amodeis say they don't "belong" to EA
```

**Decision Dynamics**:
- Technical decisions: Dario + Jared + Sam (research-driven consensus)
- Strategic partnerships: Daniela + Dario (operational + vision alignment)
- Policy positions: Jack shapes, Dario approves, Daniela executes
- Infrastructure: Tom influences, Dario decides major investments
- Safety gate: Jared (RSO) has veto power on deployment

### Optimal Engagement Sequence

**Recommended Approach**:

1. **Start with Jack Clark** (If Policy/Governance angle)
   - Most publicly accessible
   - Import AI provides engagement opportunity
   - Can validate policy compliance and strategic fit
   - Board member can influence others

2. **OR Start with Tom Brown** (If Technical/Infrastructure angle)
   - Publicly shares email (tom@anthropic.com)
   - Accessible for technical discussions
   - Can validate infrastructure value
   - Champions quality solutions

3. **Build to Jared Kaplan** (Validate Safety/Research)
   - CSO validates technical and safety approach
   - RSO can block if safety concerns
   - Academic credibility important
   - Influences research direction

4. **Engage Daniela Amodei** (Strategic Partnership Discussion)
   - President oversees partnerships
   - Validates mission alignment
   - Can authorize partnership structure
   - Focuses on cultural fit

5. **Final Decision: Dario Amodei** (CEO Approval)
   - Ultimate decision authority
   - Needs to see technical + mission + strategic fit
   - Values input from other co-founders
   - Final approver on budget and contracts

**Alternative: Research Channel**:
- Sam McCandlish for deep technical architecture discussions
- Less public-facing, best via academic introduction
- Can influence from research perspective

---

## ENGAGEMENT PRIORITY MATRIX

| Name | Title | Priority Level | Decision Type | Best Contact Method | Timing |
|------|-------|----------------|---------------|---------------------|--------|
| Dario Amodei | CEO | HIGHEST | Final Approver | Conference, warm intro, demonstrate safety focus | After validation from others |
| Daniela Amodei | President | HIGHEST | Final Approver (Partnerships) | Strategic meetings, demonstrate mission alignment | Mid-stage partnership |
| Jared Kaplan | CSO/RSO | HIGH | Safety Veto + Strong Influence | Technical conferences, research forums | Early validation |
| Jack Clark | Policy Head | HIGH | Strategic Influence | Import AI engagement, policy forums, email | Early validation (policy) |
| Tom Brown | Compute Chief | MEDIUM-HIGH | Technical Influence | Email (tom@anthropic.com), technical forums | Early validation (infra) |
| Sam McCandlish | Chief Architect | MEDIUM | Research Influence | Academic channels, research conferences | Deep technical validation |

---

## FINAL STRATEGIC RECOMMENDATIONS

### Universal Engagement Principles

**ALWAYS**:
1. Lead with safety and mission alignment (not profit)
2. Demonstrate research-backed, evidence-based approach
3. Acknowledge limitations honestly
4. Reference Constitutional AI methodology
5. Show understanding of Responsible Scaling Policy
6. Emphasize long-term value over short-term gains
7. Demonstrate cultural and values fit with PBC mission

**NEVER**:
1. Pure profit maximization pitch
2. Hype without substance
3. Benchmark gaming or vanity metrics
4. Pressure tactics or rushing decisions
5. Ignoring safety or governance requirements
6. Attacking competitors
7. Dismissing policy concerns

### Key Success Factors

1. **Safety First**: If it's not safe, it's not viable for Anthropic
2. **Mission Aligned**: Must advance "reliable, interpretable, steerable AI"
3. **Evidence-Based**: Research and proof, not claims
4. **Transparency**: Explainable and auditable
5. **Long-term**: Sustainable value, not quick wins
6. **Cultural Fit**: Values-aligned team and approach

---

**Document Status**: Complete - All 6 key decision-makers profiled with comprehensive social media research
**Next Steps**: Review 02b_social_media_flashcards.md for quick-reference format
**Maintenance**: Update quarterly or when major role changes occur
